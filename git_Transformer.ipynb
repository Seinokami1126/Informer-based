{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73732c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\17306\\miniconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  lr 0.312500 |  loss 0.12912| acc 1.000000\n",
      "| epoch   1 |  lr 0.312500 |  loss 0.12909| acc 1.000000\n",
      "| epoch   1 |  lr 0.312500 |  loss 0.13076| acc 1.000000\n",
      "| epoch   1 |  lr 0.312500 |  loss 0.12788| acc 1.000000\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 11.54s | test_acc: 0.998125 |  \n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  lr 0.312500 |  loss 0.14937| acc 0.987500\n",
      "| epoch   2 |  lr 0.312500 |  loss 0.14901| acc 0.987500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 294>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    292\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT_85.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 295\u001b[0m     \u001b[43mtrain_informer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mtrain_informer\u001b[1;34m()\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m89\u001b[39m)\n\u001b[0;32m    267\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 268\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlost\u001b[49m\u001b[43m,\u001b[49m\u001b[43macr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlossfix\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_data, lost, acr, model, lossfix, lr, optimizer, scheduler, epochs, epoch)\u001b[0m\n\u001b[0;32m    213\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    214\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 215\u001b[0m lost\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    216\u001b[0m acr\u001b[38;5;241m.\u001b[39mappend(accuracy(output,targets))\n\u001b[0;32m    217\u001b[0m log_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_data) \u001b[38;5;241m/\u001b[39m batch_size \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math as ms\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from d2l import torch as d2l\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from matplotlib.animation import FuncAnimation\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "\n",
    "# This concept is also called teacher forceing.\n",
    "# The flag decides if the loss will be calculted over all\n",
    "# or just the predicted values.\n",
    "calculate_loss_over_all_values = False\n",
    "cypl = 200\n",
    "batch_size = 80 # batch size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"1.同时产生多D数据污染\n",
    "2.对D需要进行编码\n",
    "\"\"\"\n",
    "\n",
    "def get_data():\n",
    "    data = pd.read_csv('E:/论文/数据/fx3/wz50_pos_noise_35.0000.csv', encoding='GBK', chunksize=100,header = None) \n",
    "    data = pd.concat(data, ignore_index=True)\n",
    "    data = data.loc[:, :]\n",
    "\n",
    "    train_data, test_data = train_test_split(data, train_size=0.8, random_state=np.random.randint(1,100))\n",
    "\n",
    "    train_data =torch.tensor(np.array(train_data)).float()\n",
    "#     train_label =train_data[:,-1].reshape(-1,1)\n",
    "#     train_seq = train_data[:,0:-1] \n",
    "\n",
    "    test_data =torch.tensor(np.array(test_data)).float()\n",
    "#     test_label =test_data[:,-1].reshape(-1,1)\n",
    "#     test_seq = test_data[:,0:-1] \n",
    "\n",
    "\n",
    "#     train_sequence = torch.cat((train_seq,train_label),dim =1).float()\n",
    "#     test_sequence = torch.cat((test_seq,test_label),dim =1).float()\n",
    "#     train_sequence = train_sequence\n",
    "#     test_sequence = test_sequence\n",
    "    return train_data, test_data\n",
    "def input_trans(data,batch_size):\n",
    "    data = data.reshape(cypl,batch_size)\n",
    "#     data = np.array(data.cpu()).T\n",
    "#     data = torch.tensor(np.diff(data)).t()\n",
    "    data=data.cpu()\n",
    "#     data = torch.cat((data,torch.zeros([1,len(data[0])])),dim = 0)\n",
    "    \n",
    "#     data = torch.clamp(data,min=0.0)\n",
    "    data = data.reshape(cypl,batch_size,1).to(device)\n",
    "    return data\n",
    "\n",
    "def noramlization(data):\n",
    "    \n",
    "    return (data - torch.mean(data))/torch.std(data)\n",
    "\n",
    "def get_batch(source,i, batch_size):\n",
    "  #  seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    if i + batch_size > len(source):  # 如果不够，直接返回None\n",
    "        return None, None\n",
    "    else:\n",
    "        seq_len = batch_size\n",
    "        data = source[i:i + seq_len]\n",
    "        target = data[:,-1]\n",
    "        data1 = data[:,0:cypl]\n",
    "        data1 = data1.t().to(device)\n",
    "        target = target.reshape(1,batch_size).to(device)\n",
    "        return data1,target\n",
    "def time_embedding_pos(data,pos):\n",
    "    data = data.reshape(data.shape[0],data.shape[1])\n",
    "    data =data.t()\n",
    "    pos = pos.transpose(0,1)\n",
    "    sfr = data\n",
    "    box = torch.zeros(data.shape[0],data.shape[1],cypl)\n",
    "    box[:,:,0] = sfr\n",
    "    for j in range(len(box[0,:,0])):\n",
    "        box[:,j,1:j+1] = torch.flip(box[:,0:j,0], dims=[1])\n",
    "    box = box.to(device)\n",
    "    posi = pos[:,:,1:]\n",
    "    \n",
    "    box[:,:,1:] += posi\n",
    "    box = box.transpose(0,1).to(device)\n",
    "    return box\n",
    "def dis(src):\n",
    "    d = src.shape[0]\n",
    "    for i in range(d):\n",
    "        src[i,:,1:] = src[i,:,1:] * src[i,-1,0]\n",
    "    return src\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=cypl):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-ms.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "     \n",
    "        fd = x + self.pe[:x.size(0), :]\n",
    "        \n",
    "        return fd\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=12, num_layers=6, dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(cypl)\n",
    "        self.lru = nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
    "        \n",
    "        self.Relu=nn.ReLU()\n",
    "        self.maxpooling = torch.nn.MaxPool1d(6, stride=1, padding=1)\n",
    "        self.BN = nn.BatchNorm1d(300)\n",
    "        self.cov1 = nn.Conv1d(in_channels=cypl,out_channels=200,kernel_size=10,padding = 8)\n",
    "        self.cov2 = nn.Conv1d(in_channels=200,out_channels=100,kernel_size=3,padding = 2)\n",
    "        self.cov3 = nn.Conv1d(in_channels=100,out_channels=12,kernel_size=1,padding = 0)\n",
    "        self.cov_block = nn.Sequential(self.cov1,self.Relu,self.maxpooling,\n",
    "                                       self.cov2,self.Relu,self.maxpooling,\n",
    "                                       self.cov3,self.Relu,self.maxpooling)\n",
    "#         self.cov4 = nn.Conv1d(in_channels=300,out_channels=200,kernel_size=3,padding = 1)\n",
    "#         self.cov5 = nn.Conv1d(in_channels=200,out_channels=100,kernel_size=3,padding = 1)\n",
    "#         self.cov6 = nn.Conv1d(in_channels=100,out_channels=10,kernel_size=3,padding = 1)\n",
    "\n",
    "#         self.cov_block1 = nn.Sequential(self.cov4,self.Relu,self.maxpooling,\n",
    "#                                        self.cov5,self.Relu,self.maxpooling,\n",
    "#                                        self.cov6,self.Relu,self.maxpooling)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=6, dropout=0,activation = \"gelu\",\n",
    "                                                       batch_first = False,norm_first = False,dim_feedforward = 200)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.decoder = nn.Sequential(nn.Linear(12, 5),nn.Linear(5, 1))\n",
    "        self.decoder2 = nn.Sequential(nn.Linear(cypl,120),nn.Tanh(),\n",
    "                                      nn.Linear(120,100),nn.Tanh(),\n",
    "                                      nn.Linear(100,10),nn.Tanh(),\n",
    "                                      nn.Linear(10,2),nn.Tanh()\n",
    "                                     )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.4\n",
    "        \n",
    "        self.decoder[0].bias.data.zero_()\n",
    "        self.decoder[0].weight.data.uniform_(-initrange, initrange)\n",
    "        for i in range(0,len(self.decoder2),2):\n",
    "            self.decoder2[i].bias.data.zero_()\n",
    "            self.decoder2[i].weight.data.uniform_(-initrange, initrange)\n",
    "    def forward(self, src):\n",
    "\n",
    "\n",
    "        output = self.pos_encoder(src)\n",
    "#         output = dis(output)\n",
    "#         src = output.transpose(0,2).transpose(0,1)\n",
    "\n",
    "        output = self.cov_block(output.transpose(0,2).transpose(0,1))\n",
    "\n",
    "#         output = output +src\n",
    "#         output size = (batch_size,emb,time len )\n",
    "#         output = self.cov_block1(output)\n",
    "\n",
    "        output = self.transformer_encoder(output.transpose(0,2).transpose(1,2)) \n",
    "\n",
    "#         output = output.transpose(0,2).transpose(1,2)\n",
    "\n",
    "        output = self.decoder(output.transpose(0,1)).squeeze(2)\n",
    "#         output = noramlization(output)\n",
    "\n",
    "\n",
    "        \n",
    "#         output = output.squeeze(2)\n",
    "    \n",
    "        output = self.decoder2[0](output.float())\n",
    "\n",
    "        for i in range(1,len(self.decoder2)):\n",
    "            \n",
    "            output = self.decoder2[i](output)\n",
    "            output = noramlization(output)\n",
    "            \n",
    "\n",
    "        \n",
    "        return output\n",
    "\n",
    "def train(train_data,lost,acr,model,lossfix,lr,optimizer,scheduler,epochs,epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        \n",
    "        data, targets = get_batch(train_data, i, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        data = input_trans(data,batch_size) \n",
    "        data = noramlization(data)\n",
    "        output = model(data).float()\n",
    "        targets = targets.reshape([batch_size]).long()\n",
    "        loss = lossfix(output,targets).sum()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        lost.append(float(loss))\n",
    "        acr.append(accuracy(output,targets))\n",
    "        log_interval = int(len(train_data) / batch_size / 4)\n",
    "        total_loss += loss.item()\n",
    "#             cur_loss = total_loss / log_interval\n",
    "        elapsed = time.time() - start_time\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            print('| epoch {:3d} |  '\n",
    "                  'lr {:02.6f} |  '\n",
    "\n",
    "                  'loss {:5.5f}| acc {:2f}'.format(\n",
    "                epoch,   scheduler.get_lr()[0],\n",
    "\n",
    "                loss,accuracy(output,targets)))  # , math.exp(cur_loss)\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "def accuracy(y_hat, y):\n",
    "    m = nn.Softmax(dim = 1)\n",
    "    y_hat = m(y_hat)\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return round(float(cmp.type(y.dtype).sum())/len(y.t()),6)\n",
    "   # pre_output = y_hat[torch.tensor(range(0,len(y_hat))).type(torch.int64),y.type(torch.int64)]\n",
    "   #  return pre_output\n",
    "\n",
    "def train_informer():\n",
    "    lossfix=nn.CrossEntropyLoss()\n",
    "    lossfix = lossfix.to(device)\n",
    "    model = TransAm().to(device)\n",
    "    lr = 1*batch_size/256\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.75)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs = 100 # The number of epochs\n",
    "    best_model = None\n",
    "    lost = []\n",
    "    acr = []\n",
    "    ts_lost = []\n",
    "    train_data, test_data = get_data()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "\n",
    "        train_data=train_data[torch.randperm(train_data.size(0))]\n",
    "\n",
    "\n",
    "        print('-' * 89)\n",
    "\n",
    "        print('-' * 89)\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_data,lost,acr,model,lossfix,lr,optimizer,scheduler,epochs,epoch)\n",
    "\n",
    "        test_acc = 0\n",
    "\n",
    "        for i in range(20):\n",
    "\n",
    "            tst,tar = get_batch(test_data,np.random.randint(1,len(test_data) - batch_size - 1),batch_size)\n",
    "\n",
    "            tst = input_trans(tst,batch_size)\n",
    "            tst = noramlization(tst)\n",
    "            tst_output = model(tst).float()\n",
    "            test_acc += accuracy(tst_output,tar)\n",
    "        ts_lost.append(test_acc/20) \n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | test_acc: {:5.6f} |  '.format(\n",
    "            epoch, (time.time() - epoch_start_time),test_acc/20\n",
    "           ))  # , math.exp(val_loss) | valid ppl {:8.2f}\n",
    "        print('-' * 89)\n",
    "\n",
    "        if test_acc/20 >=0.99 :\n",
    "            torch.save(model, 'T_99.pt')\n",
    "        if test_acc/20 >= 0.95 and test_acc/20<0.99:\n",
    "            torch.save(model, 'T_95.pt')\n",
    "        if test_acc/20 >=0.85 and test_acc/20<0.95:\n",
    "            torch.save(model, 'T_85.pt')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    train_informer()\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
